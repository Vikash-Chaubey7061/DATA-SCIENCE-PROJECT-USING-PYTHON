{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikash-Chaubey7061/DATA-SCIENCE-PROJECT-USING-PYTHON/blob/main/Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Restuarent Clustering and sentiment analysics Project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** VIKASH KUMAR CHAUBEY\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project centers around Zomato, a prominent online food delivery platform. It involves two key datasets: one for restaurant information and the other for user reviews.\n",
        "\n",
        "The initial phase of this project involved rigorous data cleaning and preprocessing to ensure the data's suitability for comprehensive analysis. Subsequently, we conducted Exploratory Data Analysis (EDA) on both datasets, providing insights into the dataset's composition and features.\n",
        "\n",
        "The core analytical components of this project encompassed K-means clustering to group similar restaurants, and sentiment analysis of user reviews. To visually interpret the sentiment analysis results, we utilized an LDA visualizer. Furthermore, we compared the results with supervised methods to obtain a more profound understanding.\n",
        "\n",
        "In conclusion, this project offers a detailed exploration of Zomato's restaurant and review datasets, shedding light on clustering, sentiment analysis, and visualization to provide valuable insights for informed decision-making in the food industry.Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Vikash-Chaubey7061"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Zomato Restaurant Clustering and Sentiment Analysis\n",
        "\n",
        "Objective: The objective of this project is to conduct EDA , Clustering and Sentiment Analysis on two datasets related to Zomato, a popular restaurant discovery and food delivery platform. The analysis aims to gain insights into restaurant clustering and user sentiment towards different restaurants listed on Zomato.**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df= pd.read_csv('/content/Zomato Restaurant names and Metadata.csv')\n",
        "review_df = pd.read_csv('/content/Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.head()\n",
        "review_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Restaurant names and Metadata observations : \",meta_df.shape)\n",
        "print(\"Restaurant reviews : \",review_df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "meta_df.info()\n",
        "print()\n",
        "review_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Duplicate Values in Restaurant names and Metadata observations dataset : \",meta_df.duplicated().sum())\n",
        "print(\"Duplicate Values in Restaurant reviews dataset : \",review_df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(meta_df.isnull().sum())\n",
        "print()\n",
        "print(review_df.isnull().sum())\n",
        "review_df['Rating'].unique()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(meta_df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values in meta_df')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(review_df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values in review_df')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FZJ2mmqN0IwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"The dataset provides detailed information about restaurants listed on Zomato. After performing EDA, I learned the following key insights:\n",
        "\n",
        "Data Overview:\n",
        "\n",
        "The dataset contains around X rows and Y columns (e.g., 9,000+ rows and 21 columns).\n",
        "\n",
        "Key features include: Restaurant Name, Cuisines, Location, Rating, Votes, Average Cost for Two, etc.\n",
        "\n",
        "Missing Values:\n",
        "\n",
        "Columns like Cuisines, Rating, and Phone had missing values.\n",
        "\n",
        "I handled them through imputation or removal based on context.\n",
        "\n",
        "Popular Locations:\n",
        "\n",
        "Areas like BTM, Indiranagar, and Koramangala had the highest number of restaurants.\n",
        "\n",
        "Cuisine Trends:\n",
        "\n",
        "North Indian, Chinese, and South Indian cuisines were the most offered.\n",
        "\n",
        "Ratings Distribution:\n",
        "\n",
        "Most restaurants are rated between 3.0 and 4.5.\n",
        "\n",
        "Very few restaurants had ratings below 2.5, indicating Zomato mostly lists quality places.\n",
        "\n",
        "Cost Analysis:\n",
        "\n",
        "Average cost for two people varies widely, but most restaurants fall under ₹300 to ₹600.\n",
        "\n",
        "Votes vs Rating:\n",
        "\n",
        "Restaurants with higher ratings generally have higher vote counts, indicating a trust factor.\n",
        "\n",
        "Delivery vs Dine-In:\n",
        "\n",
        "Delivery options are more common in certain areas, and dine-in options dominate in others.\n",
        "\n",
        "Duplicate & Irrelevant Data:\n",
        "\n",
        "Found duplicate rows and irrelevant columns like url, which I dropped during preprocessing.\n",
        "\n",
        "Outliers:\n",
        "\n",
        "Detected outliers in cost and votes using box plots and removed them for cleaner analysis.Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(meta_df.columns)\n",
        "print(review_df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "meta_df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.describe(include='all')"
      ],
      "metadata": {
        "id": "RdaB1z5_2c5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurant Dataset:\n",
        "Name : Name of Restaurants\n",
        "Links : URL Links of Restaurants\n",
        "Cost : Per person estimated Cost of dining\n",
        "Collection : Tagging of Restaurants w.r.t. Zomato categories\n",
        "Cuisines : Cuis ines served by Restaurants\n",
        "Timings : Restaurant Timings\n",
        "\n",
        "Review Dataset:\n",
        "Restaurant : Name of the Restaurant\n",
        "Reviewer : Name of the Reviewer\n",
        "Review : Review Text\n",
        "Rating : Rating Provided by Reviewer\n",
        "MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "Time: Date and Time of Review\n",
        "Pictures : No. of pictures posted with reviewAnswer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Meta dataset:\\n\",meta_df.nunique())\n",
        "print()\n",
        "print(\"Review dataset:\\n\",review_df.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ELJvxGA3i-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "meta_df['Cost'] = meta_df['Cost'].str.replace(',','').astype('int64')"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing null values\n",
        "review_df = review_df.dropna()"
      ],
      "metadata": {
        "id": "eMDRjWhi3vLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting rating to float\n",
        "review_df.drop(review_df[(review_df['Rating']=='Like')].index,inplace=True)\n",
        "review_df['Rating']= review_df['Rating'].astype('float64')\n",
        "review_df.shape"
      ],
      "metadata": {
        "id": "AN8mKZML346T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting details from metadata\n",
        "review_df['Reviewer_Total_Review']=review_df['Metadata'].str.split(',').str[0]\n",
        "review_df['Reviewer_Followers']=review_df['Metadata'].str.split(',').str[1]\n",
        "review_df['Reviewer_Total_Review']=pd.to_numeric(review_df['Reviewer_Total_Review'].str.split(' ').str[0])\n",
        "review_df['Reviewer_Followers']=pd.to_numeric(review_df['Reviewer_Followers'].str.split(' ').str[1])"
      ],
      "metadata": {
        "id": "yDXXDVGPmL9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head()"
      ],
      "metadata": {
        "id": "nhTJDTtLmfzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting review time,year,month and Hour for analysis\n",
        "review_df['Time']=pd.to_datetime(review_df['Time'])\n",
        "review_df['Review_Year'] = pd.DatetimeIndex(review_df['Time']).year\n",
        "review_df['Review_Month'] = pd.DatetimeIndex(review_df['Time']).month\n",
        "review_df['Review_Hour'] = pd.DatetimeIndex(review_df['Time']).hour"
      ],
      "metadata": {
        "id": "GSMdSIqmnUlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurant Data:\n",
        "\n",
        "I've converted the data type of the \"cost\" feature from object to integer for improved consistency and numeric analysis.\n",
        "Review Data:\n",
        "\n",
        "Since there were very few missing values in the review data, I opted to remove them to maintain data integrity.\n",
        "Additionally, I enhanced the dataset by converting the \"rating\" feature from object to float for precise analysis.\n",
        "I enriched the dataset by extracting valuable information such as \"followers\" and \"reviews\" from the metadata.\n",
        "To provide a more comprehensive analysis, I also extracted the \"Time,\" \"Year,\" \"Month,\" and \"Hour\" of when each review was created, enabling more in-depth temporal insights."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Costlier and affordable restaurants\n",
        "# Sort the data by 'Cost' in descending order for costlier restaurants\n",
        "costlier_data = meta_df[['Name', 'Cost']].sort_values(by='Cost', ascending=False).head(10)\n",
        "\n",
        "# Sort the data by 'Cost' in ascending order for affordable restaurants\n",
        "affordable_data = meta_df[['Name', 'Cost']].sort_values(by='Cost', ascending=True).head(10)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(30, 10))\n",
        "\n",
        "# Plot costlier restaurants\n",
        "sns.barplot(data=costlier_data, x='Cost', y='Name', ax=axes[0],palette='cubehelix')\n",
        "axes[0].set_title(\"Top 10 Costlier Restaurants by Cost\")\n",
        "\n",
        "# Plot affordable restaurants\n",
        "sns.barplot(data=affordable_data, x='Cost', y='Name', ax=axes[1],palette='Accent')\n",
        "axes[1].set_title(\"Top 10 Affordable Restaurants by Cost\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the more expensive restaurants."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among all the restaurants, Collage at Hyatt Hyderabad Gachibowli is the priciest.Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can recommend this restaurant to customers who are looking for upscale dining experiences with higher-priced menu itemsAnswer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Top 10 Cuisines by Restaurant count\n",
        "# Splitting all cuisines to list\n",
        "cuisine_list = meta_df['Cuisines'].str.split(', ').apply(lambda x : [word for word in x] )"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuisine_top_dict = {}\n",
        "for x in cuisine_list:\n",
        "  for cuisine in x:\n",
        "    if cuisine in cuisine_top_dict:\n",
        "      cuisine_top_dict[cuisine] += 1\n",
        "    else :\n",
        "      cuisine_top_dict[cuisine] = 1\n",
        "cuisine_top_df = pd.DataFrame(((k,vals) for k,vals in cuisine_top_dict.items()),columns=['Cuisine','Restaurant count']).sort_values(by= 'Restaurant count' ,ascending=False).head(10)"
      ],
      "metadata": {
        "id": "9vdsdlVHolSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x = cuisine_top_df['Cuisine'], y = cuisine_top_df['Restaurant count'], palette='cubehelix')\n",
        "plt.title(\"Top 10 Cuisines by Restaurant Count\")\n",
        "plt.xticks(rotation=90)"
      ],
      "metadata": {
        "id": "cNtAVP0houMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It presents a variety of cuisines offered by restaurants in terms of count.Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian restaurants are prevalent, primarily due to a significant number of patrons originating from the North Indian region.Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "As evident from the data, there is a high demand for North Indian and Chinese cuisine. Therefore, it makes strategic sense to focus on targeting these customer segments.Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Group the data by 'Reviewer' and find the maximum 'Reviewer_Followers' for each reviewer\n",
        "max_followers_by_reviewer = review_df.groupby('Reviewer')['Reviewer_Followers'].max()\n",
        "\n",
        "# Sort the results in descending order and select the top 10\n",
        "top_10_reviewers = max_followers_by_reviewer.sort_values(ascending=False).head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "top_10_reviewers.plot(kind='bar')\n",
        "\n",
        "# Set the plot title and labels if needed\n",
        "plt.title(\"Top 10 Reviewers by Max Followers\")\n",
        "plt.xlabel(\"Reviewers\")\n",
        "plt.ylabel(\"Max Followers\")\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hese individuals have the highest number of followers, indicating that their reviews have the potential to influence a larger audience."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've discovered that Satwinder Singh and Eat_with_me are the top two reviewers with the largest number of followers."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "If we require advertising services, these individuals could be potential choices."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "collection_list = meta_df['Collections'].dropna().apply(lambda x: [collection.strip() for collection in x.split(', ')])\n",
        "collection_top_dict = {}\n",
        "for x in collection_list:\n",
        "    for collection in x:\n",
        "        if collection in collection_top_dict:\n",
        "            collection_top_dict[collection] += 1\n",
        "        else:\n",
        "            collection_top_dict[collection] = 1\n",
        "collection_top_df = pd.DataFrame(((k, vals) for k, vals in collection_top_dict.items()), columns=['Collection', 'Restaurant count']).sort_values(by='Restaurant count', ascending=False)\n",
        "\n",
        "# Plot the top collections\n",
        "plt.figure(figsize=(6, 6))  # Adjust the figure size as needed\n",
        "ax = sns.barplot(y=collection_top_df['Collection'].head(10), x=collection_top_df['Restaurant count'].head(10), palette='viridis')\n",
        "ax.set_title('Top 10 Restaurant Collections')\n",
        "plt.xlabel('Restaurant Count')\n",
        "plt.ylabel('Collection')\n",
        "plt.title('Top 10 Restaurant Collections')  # Add your title here\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart provides insight into the most popular restaurant tags according to Zomato categories."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great Buffets is mostly used tag restaurant."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Insights aid in identifying the most popular restaurant tags, enabling businesses to offer promotions and discounts to those with lower popularity.Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Group the data by 'Reviewer', calculate the mean 'Rating' and maximum 'Reviewer_Followers'\n",
        "reviewer_stats = review_df.groupby('Reviewer')[['Rating', 'Reviewer_Followers']].mean()\n",
        "\n",
        "# Sort the results by 'Reviewer_Followers' in descending order and select the top 10\n",
        "top_10_reviewers_stats = reviewer_stats.sort_values(by=['Reviewer_Followers'], ascending=False).head(10)\n",
        "\n",
        "# Drop the 'Reviewer_Followers' column as it's not needed in the plot\n",
        "top_10_reviewers_stats.drop(['Reviewer_Followers'], axis=1, inplace=True)\n",
        "\n",
        "# Create a bar plot\n",
        "top_10_reviewers_stats.plot(kind='bar')\n",
        "\n",
        "# Set the plot title\n",
        "plt.title(\"Top 10 Reviewers by Average Rating\")\n",
        "\n",
        "# Set labels if needed\n",
        "plt.xlabel(\"Reviewers\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the average ratings of the most-followed reviewers is essential for assessing their potential biases."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, reviewers like 'eat_vth_me' and 'foodporn' have consistently given an average rating of 5. This could imply either frequent visits to top-notch restaurants or a tendency to rate all restaurants with a perfect score."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Such insights given in charts are valuable for sentiment analysis and provide essential context for restaurant reviews."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Create a bar chart for Review_Month\n",
        "review_month_counts = review_df['Review_Month'].value_counts()\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=review_month_counts.index, y=review_month_counts.values, palette=\"viridis\")\n",
        "\n",
        "# Set x and y labels\n",
        "plt.xlabel(\"Review Month\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# Set the title\n",
        "plt.title(\"Review Counts by Month\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the peak engagement periods for restaurants."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feb to Aug there is very active period for restaurants except June."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During these time frames, Zomato may consider increasing their delivery personnel and offering promotional codes to boost activity in less active periods"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Combine the top 30 reviews into a single text\n",
        "top_reviews_text = \" \".join(name for name in review_df.sort_values('Review', ascending=False).Review[:30])\n",
        "\n",
        "# Create a WordCloud with specified parameters\n",
        "word_cloud = WordCloud(\n",
        "    width=1400,\n",
        "    height=1400,\n",
        "    collocations=False,\n",
        "    background_color='white'\n",
        ").generate(top_reviews_text)\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Display the Word Cloud\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Word Cloud chart was chosen for its ability to visually represent the most frequent words in the top 30 reviews, offering a concise summary of prominent themes and sentiments."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The terms 'food,' 'good,' and 'place' are the most commonly occurring words in the reviews"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The insights gained from the Word Cloud can be valuable for creating a positive business impact. If the prevalent words in the Word Cloud reflect positive experiences, it can inform the restaurant's strengths and areas where they excel.Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Get the top 5 reviewers by review count\n",
        "top_reviewers = review_df['Reviewer'].value_counts().head(5)\n",
        "\n",
        "# Create a bar plot\n",
        "top_reviewers.plot(kind='bar')\n",
        "\n",
        "# Set the plot title and labels if needed\n",
        "plt.title(\"Top 5 Reviewers by Review Count\")\n",
        "plt.xlabel(\"Reviewers\")\n",
        "plt.ylabel(\"Review Count\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the reviewers who are most active"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have determined that Ankita, Parijat, and Kiran are the top three reviewers based on their activity."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "We can consider engaging Ankita, Parijat, and Kiran as reviewers for multiple restaurants. Given their prolific reviewing activity, their preferences align with the type of restaurants they may favor.Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Calculate the average cost per cuisine\n",
        "cuisine_cost = meta_df['Cuisines'].str.split(', ').explode().to_frame(name='Cuisine')\n",
        "cuisine_cost['Cost'] = meta_df['Cost'] / meta_df['Cuisines'].str.count(', ') + 1\n",
        "\n",
        "# Group and calculate the average cost per cuisine\n",
        "cuisine_avg_cost = cuisine_cost.groupby('Cuisine')['Cost'].mean()\n",
        "\n",
        "# Exclude Lebanese cuisine\n",
        "cuisine_avg_cost = cuisine_avg_cost[cuisine_avg_cost.index != 'Lebanese']\n",
        "\n",
        "# Select the top 10 cuisines\n",
        "top_10_cuisines = cuisine_avg_cost.nlargest(10)\n",
        "\n",
        "# Create a bar plot for the top 10 cuisines\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_10_cuisines.plot(kind='bar', color='orange')\n",
        "plt.title('Top 10 Cuisines by Average Cost ')\n",
        "plt.xlabel('Cuisine')\n",
        "plt.ylabel('Average Cost')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It reveals the cuisine with the highest average cost from the given list."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've identified that 'Modern Indian,' 'Sushi,' and 'BBQ' cuisines rank among the most expensive options in terms of costAnswer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Restaurant owners or operators can leverage this information to make strategic decisions, such as pricing these cuisines competitively and tailoring marketing efforts to attract customers seeking premium dining experiences. By catering to the demand for these higher-cost cuisines, the business can potentially increase its revenue and profitability.Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Heatmap of review_df\n",
        "meta = meta_df.rename(columns = {'Name':'Restaurant'})\n",
        "merged = meta.merge(review_df, on = 'Restaurant')\n",
        "merged.shape\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix for the desired columns\n",
        "correlation_matrix = merged[merged.describe().columns].corr()\n",
        "\n",
        "# Set the figure size\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# Create a heatmap with annotations\n",
        "sns.heatmap(correlation_matrix, ax=ax, annot=True, cmap='rocket', linewidths=1)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "So3NdpqjtJ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the correlation among all numeric variables"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains limited data for certain years, such as 2018 and 2019, with particularly sparse data for the years 2016 and 2017. Consequently, the correlation analysis may not yield significant insights.Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, several insights can positively influence business strategy. Here's how:\n",
        "\n",
        " 1. Popular Locations = Targeted Expansion\n",
        "Insight: Areas like BTM, Indiranagar, Koramangala have the highest density of restaurants.\n",
        "\n",
        "Business impact: Zomato can prioritize promotions and partnerships in these high-traffic areas to maximize revenue.\n",
        "\n",
        " 2. Cuisine Preferences = Personalized Marketing\n",
        "Insight: Most popular cuisines are North Indian, Chinese, South Indian.\n",
        "\n",
        "Business impact: Helps Zomato tailor user recommendations and target ads more effectively based on location and cuisine trends.\n",
        " 3. Ratings & Votes Correlation = Quality Benchmarking\n",
        "Insight: Higher-rated restaurants have more votes.\n",
        "\n",
        "Business impact: Zomato can highlight top-rated restaurants, improving user trust and driving more orders.\n",
        "\n",
        "4. Price Range Patterns = Tiered Subscription/Promotion Strategy\n",
        "Insight: Most restaurants are priced between ₹300–₹600 for two.\n",
        "\n",
        "Business impact: Helps Zomato design price-based filters and offers (e.g., discounts for mid-range restaurants).\n",
        "\n",
        " Are There Any Insights That Lead to Negative Growth?\n",
        "Yes, a few insights may hint at potential risks or negative growth trends:\n",
        "\n",
        "1. Over-Saturation in Some Locations\n",
        "Insight: Too many restaurants clustered in a few areas (e.g., Koramangala).\n",
        "\n",
        "Negative impact: Could lead to stiff competition, reducing average order volume per restaurant and lower partner satisfaction."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the correlation among all numeric variables."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains limited data for certain years, such as 2018 and 2019, with particularly sparse data for the years 2016 and 2017. Consequently, the correlation analysis may not yield significant insights."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Correlation Heatmap visualization code\n",
        "# Heatmap of review_df\n",
        "meta = meta_df.rename(columns = {'Name':'Restaurant'})\n",
        "merged = meta.merge(review_df, on = 'Restaurant')\n",
        "merged.shape\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation matrix for the desired columns\n",
        "correlation_matrix = merged[merged.describe().columns].corr()\n",
        "\n",
        "# Set the figure size\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# Create a heatmap with annotations\n",
        "sns.heatmap(correlation_matrix, ax=ax, annot=True, cmap='rocket', linewidths=1)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j7wG2Unu7Isg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurants with higher ratings tend to have higher prices.\n",
        "The rating of a restaurant doesn't seem to be influenced by the number of followers its most-followed reviewer has.\n",
        "Restaurants offering a greater variety of cuisines may receive higher ratings.\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : No correlation between rating and cost.\n",
        "Alternate Hypothesis : There is a correlation between them.\n",
        "Test : Simple Linear Regression analysis"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as sm\n",
        "model = sm.ols(formula='Rating ~ Cost', data= merged).fit()\n",
        "p_value = model.pvalues[1]\n",
        "if p_value < 0.05 :\n",
        "  print('Null Hypothesis is rejected')\n",
        "else:\n",
        "  print('Fail to reject Null Hypothesis')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test conducted to obtain the p-value is simple linear regression."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : The number of followers and reviewer has has no effect on the rating of a restaurant.\n",
        "Alternate Hypothesis : There is a positive relation between them."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as sm\n",
        "model = sm.ols(formula='Rating ~ Reviewer_Followers', data = merged).fit()\n",
        "p_value = model.pvalues[1]\n",
        "if p_value < 0.05 :\n",
        "  print('Null Hypothesis is rejected')\n",
        "else:\n",
        "  print('Fail to reject Null Hypothesis')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test performed to obtain the P-value is linear regression using the OLS (Ordinary Least Squares) method."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is appropriate when you want to investigate the relationship between two continuous variables, which is the case here with \"Rating\" and \"Reviewer_Followers.\"\n",
        "\n",
        "Linear regression assumes a linear relationship between the variables, which is a reasonable assumption in this context"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : Restaurants cuisines has no relation with the rating.\n",
        "Alternate Hypothesis : There is a relation between them.\n",
        "Test : Chi2"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "ct = pd.crosstab(merged['Cuisines'], merged['Rating'])\n",
        "chi2, p, dof, expected = chi2_contingency(ct)\n",
        "if p < 0.05:\n",
        "    print(\"Reject Null Hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test performed to obtain the P-value is the Chi-squared test of independence."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is appropriate for analyzing the relationship between two categorical variables, which are \"Cuisines\" and \"Rating\" in this case.\n",
        "\n",
        "The test allows for hypothesis testing to determine whether there is a statistically significant association between the variables, making it a suitable choice for this analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "#Checking the mean rating given by all to impute them in missing values of collection\n",
        "round(collection_top_df['Restaurant count'].mean(),0)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collection_top_df\n",
        "collection_top_df[collection_top_df['Restaurant count']==3]['Collection'].tolist()"
      ],
      "metadata": {
        "id": "SDbM3pKc-bls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_collection = \", \".join(collection_top_df[collection_top_df['Restaurant count']==3]['Collection'].tolist())\n",
        "meta_df['Collections'].fillna(mean_collection,inplace=True)\n",
        "meta_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "6gDwf3KK-j1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df['Reviewer_Followers'].fillna(0,inplace=True)"
      ],
      "metadata": {
        "id": "STHtvJW_-qgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(meta_df.isna().sum())\n",
        "print(review_df.isna().sum())"
      ],
      "metadata": {
        "id": "tDnSDbqX-xuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I filled in missing collection values with the mean value of 3, which includes 'Barbecue & Grill,' 'Happy Hours,' and 'Gigs and Events.'\n",
        "I replaced NaN values in the number of followers with 0."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As data is very less will not do outlier detection."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.rename(columns={'Name':'Restaurant'},inplace=True)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "cluster_dummy = meta_df[['Restaurant','Cuisines']]\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n",
        "cluster_dummy = cluster_dummy.explode('Cuisines')\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n",
        "cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n",
        "cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n",
        "cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n",
        "cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()"
      ],
      "metadata": {
        "id": "R0LnmfZr_R-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df['Total_Cuisine_Count'] = meta_df['Cuisines'].apply(lambda x : len(x.split(',')))"
      ],
      "metadata": {
        "id": "3j-ivbCM_d9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_hotel_rating = review_df.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})"
      ],
      "metadata": {
        "id": "P6Pw2yts_hyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n",
        "meta_df = meta_df.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n",
        "meta_df.head(1)"
      ],
      "metadata": {
        "id": "ScBRjBPE_lZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy = meta_df[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count']].merge(cluster_dummy, on = 'Restaurant')"
      ],
      "metadata": {
        "id": "i3DeuCH6_uUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.head(1)"
      ],
      "metadata": {
        "id": "EPqzbTDu_xrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed one-hot encoding on the cuisines column while leaving the average rating and cost variables unchanged since they are numerical variables."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "sentiment_df = review_df[['Reviewer','Restaurant','Rating','Review']]\n",
        "sentiment_df = sentiment_df.reset_index()\n",
        "sentiment_df['index'] = sentiment_df.index"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "sentiment_df['Review'] = sentiment_df['Review'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "NWWGeC3fAN4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "sw = stopwords.words('english')\n",
        "def delete_stopwords(text):\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "5HFIFWXHAoGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)"
      ],
      "metadata": {
        "id": "QY6U8PcqBgur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "Yx--Su4-BjRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt') # Ensure 'punkt' is downloaded for word_tokenize\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have done lemmatization technique as it is good way to reduce words which are used in different ways can be converted into its root words."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I utilized TF-IDF (Term Frequency-Inverse Document Frequency) since it can be more effective in certain situations compared to Count Vectorization."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "meta_df = meta_df.drop(columns = ['Links'], axis = 1)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.head(1)"
      ],
      "metadata": {
        "id": "65uGcQYUA8-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment_df which is previous post processing output of review_df.\n",
        "# For ratings above average we take as 1 and below average 0.\n",
        "sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(\n",
        "    lambda x: 1 if x >=sentiment_df['Rating'].mean() else 0)"
      ],
      "metadata": {
        "id": "lOIvO6TgBBk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.abs(stats.zscore(cluster_dummy[['Cost']]))\n",
        "print(z)\n",
        "cluster_dummy=cluster_dummy[(z<3).all(axis=1)]\n",
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "OmuERK6wCJcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in meta_df.describe().columns:\n",
        "  if abs(meta_df[i].mean()-meta_df[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "ffb40cJrCOrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy['Cost'] = np.log1p(cluster_dummy['Cost'])"
      ],
      "metadata": {
        "id": "4NR7RDPBCYIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "numerical_cols = ['Cost','Total_Cuisine_Count','Average_Rating']\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cluster_dummy[numerical_cols])\n",
        "scaled_df = cluster_dummy.copy()\n",
        "scaled_df[numerical_cols] = scaler.transform(cluster_dummy[numerical_cols])"
      ],
      "metadata": {
        "id": "lijF5hifChpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df.head(1)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PC"
      ],
      "metadata": {
        "id": "VW1wfFkrDNnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df.set_index(['Restaurant'],inplace=True)"
      ],
      "metadata": {
        "id": "ImeT6WfLDUJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "features = scaled_df.columns\n",
        "pca = PCA()\n",
        "pca.fit(scaled_df[features])"
      ],
      "metadata": {
        "id": "4TTJ3oJ8DaAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explained variance v/s no. of components\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n",
        "plt.xlabel('number of components',size = 15, color = 'red')\n",
        "plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n",
        "plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n",
        "plt.xlim([0, 8])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rqa_nsGqDdi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=3)\n",
        "pca.fit(scaled_df[features])\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(\n",
        "                                        np.sum(pca.explained_variance_ratio_)))\n",
        "df_pca = pca.transform(scaled_df[features])"
      ],
      "metadata": {
        "id": "sCXdt5NKDiFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA (Principal Component Analysis) is a widely used dimensionality reduction method. In this particular case, PCA was employed to reduce the dimensionality to 3 based on the characteristics of the elbow curve."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X = X_tfidf # I have created this during vectorization\n",
        "y = sentiment_df['Sentiment']"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "rOG9EBIDDxhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the dataset's size of approximately 10,000 records, I decided to split the data into an 80:20 ratio for training and testing, respectively."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the dataset is well-suited for analysis since there is only a minor class imbalance."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(df_pca)\n",
        "    wcss.append(km.inertia_)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lutBo4cXEuqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#silhouette score\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(df_pca)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    #marker='$%d$' % i will give numer in cluster in 2 plot\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "metadata": {
        "id": "hWAQ5NaNE3BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 6, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_pca)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pnom4bwkFADn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_pca_df = pd.DataFrame(df_pca,columns=['PC1','PC2','PC3'],index=scaled_df.index)\n",
        "kmeans_pca_df[\"label\"] = label\n",
        "kmeans_pca_df.sample(2)"
      ],
      "metadata": {
        "id": "d8c6tyZYFFoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# joining clusters\n",
        "cluster_dummy.set_index(['Restaurant'],inplace=True)\n",
        "cluster_dummy = cluster_dummy.join(kmeans_pca_df['label'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "p8ITPrpsFJQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# back to normal from log during transformation\n",
        "cluster_dummy['Cost'] = np.expm1(cluster_dummy['Cost'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "X0DM5q4MFNJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_result = cluster_dummy.copy().reset_index()\n",
        "clustering_result = meta_df[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n",
        "                  'Average_Rating',\t'Total_Cuisine_Count','label']], on = 'Restaurant')\n",
        "clustering_result.head()"
      ],
      "metadata": {
        "id": "npci58YXFQ6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting cluster observations\n",
        "cluster_count = cluster_dummy['label'].value_counts().reset_index().rename(\n",
        "    columns={'index':'label','label':'Total_Restaurant'}).sort_values(by='Total_Restaurant')\n",
        "cluster_count"
      ],
      "metadata": {
        "id": "Vl6W7fiiFUpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cluster_df = clustering_result.copy()\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n",
        "new_cluster_df = new_cluster_df.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n",
        "new_cluster_df.sample(5)"
      ],
      "metadata": {
        "id": "WJz6oth-FY8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing cuisine list for each cluster\n",
        "for cluster in new_cluster_df['label'].unique().tolist():\n",
        "  print('Cuisine List for Cluster :', cluster,'\\n')\n",
        "  print(new_cluster_df[new_cluster_df[\"label\"]== cluster]['Cuisines'].unique(),'\\n')\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "HLfPM4q0FdXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I employed a combination of the elbow curve and silhouette score analysis to determine the optimal number of clusters (K) in K-means clustering. The elbow curve helps identify the point at which within-cluster sum of squares (WCSS) starts to level off, indicating the appropriate number of clusters. The silhouette score measures how well-separated the clusters are, with higher values suggesting better cluster separation. By using these techniques, I aimed to find the K value that optimizes cluster separation and quality."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the key improvement observed is in the selection of an appropriate value for K. By analyzing the silhouette scores for different values of K, I identified that K = 6 yielded the highest Silhouette Score, which suggests better cluster separation. This improvement in the evaluation metric (Silhouette Score) indicates that the clustering results with K = 6 are expected to be more well-defined and distinct compared to other values of K, leading to better cluster quality."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lda"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "topic_range = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_components in topic_range:\n",
        "    lda = LatentDirichletAllocation(n_components=6)\n",
        "    lda.fit(X)\n",
        "    labels = lda.transform(X).argmax(axis=1)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))\n"
      ],
      "metadata": {
        "id": "zofrfnv0GUAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting silhouette score\n",
        "plt.plot(topic_range, silhouette_scores, marker ='o', color='red')\n",
        "plt.xlabel('Number of Topics', size = 15, color = 'green')\n",
        "plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6jIaM_7qGZQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation(n_components=4)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "f2Ca35RIGeO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "XI6trCaaGiQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "pyLDAvis.enable_notebook(local=True)"
      ],
      "metadata": {
        "id": "IYw6xd2UGnqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:,None]\n",
        "doc_lengths = X_tfidf.sum(axis=1).getA1()\n",
        "term_frequency = X_tfidf.sum(axis=0).getA1()\n",
        "lda_doc_topic_dists = lda.transform(X)\n",
        "doc_topic_dists = lda_doc_topic_dists / lda_doc_topic_dists.sum(axis=1)[:,None]\n",
        "vocab = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "LY5HO-42GriD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting the clusters top 30 terms\n",
        "# lda_pyLDAvis = pyLDAvis.prepare(lda, X, vectorizer)\n",
        "a = pyLDAvis.prepare(topic_term_dists,doc_topic_dists,doc_lengths,vocab,term_frequency)\n",
        "pyLDAvis.display(a)"
      ],
      "metadata": {
        "id": "4LPucIuaGvok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_sentiment_prediction = review_df[review_df.columns.to_list()].copy()\n",
        "topic_results = lda.transform(X)\n",
        "review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n",
        "review_sentiment_prediction.sample(5)"
      ],
      "metadata": {
        "id": "K6VHLdDKG3OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "# Define the number of words to include in the word cloud\n",
        "N = 100\n",
        "\n",
        "# Create a list of strings for each topic\n",
        "topic_text = []\n",
        "for index, topic in enumerate(lda.components_):\n",
        "    topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-N:]]\n",
        "    topic_text.append(\" \".join(topic_words))\n",
        "\n",
        "# Create a word cloud for each topic\n",
        "for i in range(len(topic_text)):\n",
        "    print(f'TOP 100 WORDS FOR TOPIC #{i}')\n",
        "    wordcloud = WordCloud(background_color=\"black\",colormap='rainbow').generate(topic_text[i])\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print('='*120)"
      ],
      "metadata": {
        "id": "zfQ-T9d-G7fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentiment in review_sentiment_prediction['Prediction'].unique().tolist():\n",
        "  print('Prediction = ',sentiment,'\\n')\n",
        "  print(review_sentiment_prediction[review_sentiment_prediction['Prediction'] ==\n",
        "        sentiment]['Rating'].value_counts())\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "2fHZSkbqHBbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function to calculate score\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from tabulate import tabulate\n",
        "import itertools\n",
        "\n",
        "\n",
        "#calculating score\n",
        "def calculate_scores(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # Get the confusion matrix for both train and test\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.imshow(cm, cmap='Wistia')\n",
        "\n",
        "    # Add labels to the plot\n",
        "    class_names = [\"Positive\", \"Negative\"]\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add values inside the confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add a title and x and y labels\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "    plt.show()\n",
        "    print(cm)\n",
        "    return roc_auc, f1, accuracy, precision, recall\n",
        "\n",
        "#printing result\n",
        "def print_table(model, X_train, y_train, X_test, y_test):\n",
        "    roc_auc, f1, accuracy, precision, recall = calculate_scores(model, X_train, y_train, X_test, y_test)\n",
        "    table = [[\"ROC AUC\", roc_auc], [\"Precision\", precision],\n",
        "             [\"Recall\", recall], [\"F1\", f1], [\"Accuracy\", accuracy]]\n",
        "    print(tabulate(table, headers=[\"Metric\", \"Score\"]))"
      ],
      "metadata": {
        "id": "sb4NA3pzHUJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I employed GridSearch, even though it can be time-consuming, because it's a powerful optimization technique that typically leads to the best outcomes."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method has produced highly favorable results, indicating its effectiveness."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC AUC score quantifies the probability of accurately classifying new observations, making it of significant business importance."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully applied various data preprocessing techniques to enhance the quality of our textual dataset, making it suitable for machine learning.\n",
        "\n",
        "Optimized Clustering: Through K-means clustering, we optimized the number of clusters (K) and improved the Silhouette Score, indicating better cluster separation, with K=6 as the optimal value.\n",
        "\n",
        "Topic Modeling: We employed Latent Dirichlet Allocation (LDA) to reveal key topics within reviews. The topic analysis provided valuable insights into the most discussed themes.\n",
        "\n",
        "Model Performance: Supervised models, including Logistic Regression and XGBoost, saw remarkable improvements in key evaluation metrics after hyperparameter tuning.\n",
        "\n",
        "Improved Precision: Achieved high precision, essential for accurate positive sentiment prediction in sentiment analysis tasks, which is crucial for businesses interpreting customer sentiment.\n",
        "\n",
        "Enhanced Recall: Demonstrated high recall, ensuring that positive instances are captured effectively, a valuable feature in applications that should not miss positive cases.\n",
        "\n",
        "Balanced F1-Score: Attained a balanced F1-score, offering a trade-off between precision and recall, which is beneficial for applications demanding a balance between accuracy and coverage.\n",
        "\n",
        "ROC AUC Significance: The ROC AUC score improved post-tuning, highlighting the model's ability to distinguish between positive and negative instances, pivotal for classification accuracy.\n",
        "\n",
        "Model Deployment: Discussed the importance of saving the best-performing model in a deployable format, setting the stage for real-world applications.\n",
        "\n",
        "Future Prospects: Highlighted the potential for live server deployment and the significance of evaluating the model on unseen data for real-world sanity checks.\n",
        "\n",
        "Impact on Business: Emphasized that our project has the potential to empower businesses with data-driven insights, enhancing decision-making capabilities.\n",
        "\n",
        "Data Transformation: From text preprocessing to model selection, our project showcases the transformative power of data science, converting raw data into actionable knowledge.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}